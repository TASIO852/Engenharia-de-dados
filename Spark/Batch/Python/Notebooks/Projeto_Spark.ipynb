{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KccqfD8ujL3K"
      },
      "source": [
        "# Começando o Trabalho\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAh7l9hGlHq"
      },
      "source": [
        "## Apache Spark - Introdução"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy3ApfOoGlHt"
      },
      "source": [
        "### [Apache Spark](https://spark.apache.org/)\n",
        "\n",
        "Apache Spark é uma plataforma de computação em *cluster* que fornece uma API para programação distribuída para processamento de dados em larga escala, semelhante ao modelo *MapReduce*, mas projetada para ser rápida para consultas interativas e algoritmos iterativos.\n",
        "\n",
        "O Spark permite que você distribua dados e tarefas em clusters com vários nós. Imagine cada nó como um computador separado. A divisão dos dados torna mais fácil o trabalho com conjuntos de dados muito grandes porque cada nó funciona processa apenas uma parte parte do volume total de dados.\n",
        "\n",
        "O Spark é amplamente utilizado em projetos analíticos nas seguintes frentes:\n",
        "\n",
        "- Preparação de dados\n",
        "- Modelos de machine learning\n",
        "- Análise de dados em tempo real"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc3Adto4jWDj"
      },
      "source": [
        "### [PySpark](https://spark.apache.org/docs/3.1.2/api/python/index.html)\n",
        "\n",
        "PySpark é uma interface para Apache Spark em Python. Ele não apenas permite que você escreva aplicativos Spark usando APIs Python, mas também fornece o *shell* PySpark para analisar interativamente seus dados em um ambiente distribuído. O PySpark oferece suporte à maioria dos recursos do Spark, como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) e Spark Core.\n",
        "\n",
        "<center><img src=\"https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/img-001.png\"/></center>\n",
        "\n",
        "#### Spark SQL e DataFrame\n",
        "\n",
        "Spark SQL é um módulo Spark para processamento de dados estruturados. Ele fornece uma abstração de programação chamada DataFrame e também pode atuar como mecanismo de consulta SQL distribuído.\n",
        "\n",
        "#### Spark Streaming\n",
        "\n",
        "Executando em cima do Spark, o recurso de *streaming* no Apache Spark possibilita o uso de poderosas aplicações interativas e analíticas em *streaming* e dados históricos, enquanto herda a facilidade de uso do Spark e as características de tolerância a falhas.\n",
        "\n",
        "#### Spark MLlib\n",
        "\n",
        "Construído sobre o Spark, MLlib é uma biblioteca de aprendizado de máquina escalonável que fornece um conjunto uniforme de APIs de alto nível que ajudam os usuários a criar e ajustar *pipelines* de aprendizado de máquina práticos.\n",
        "\n",
        "#### Spark Core\n",
        "\n",
        "Spark Core é o mecanismo de execução geral subjacente para a plataforma Spark sobre o qual todas as outras funcionalidades são construídas. Ele fornece um RDD (*Resilient Distributed Dataset*) e recursos de computação na memória."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-9oWn7GlHw"
      },
      "source": [
        "## Utilizando o Spark no Windows\n",
        "\n",
        "[fonte](https://spark.apache.org/docs/3.1.2/api/python/getting_started/install.html)\n",
        "\n",
        "#### Passo 1 - Instalando o Java\n",
        "\n",
        "O PySpark requer a instalação do Java na versão 7 ou superior. Obtenha a versão mais recente clicando [aqui](https://www.java.com/pt-BR/download/). Para verificar a versão que está instalada em sua máquina execute a seguinte linha de código no seu *prompt*:\n",
        "\n",
        "```\n",
        "java -version\n",
        "```\n",
        "\n",
        "#### Passo 2 - Instalando o Python\n",
        "\n",
        "O Python deve ser instalado em sua versão 2.6 ou superior. Para obter a versão mais recente clique [aqui](https://www.python.org/downloads/windows/). Para verificar a versão do Python que está instalada em sua máquina digite o seguinte comando em seu *prompt*:\n",
        "\n",
        "```\n",
        "python --version\n",
        "```\n",
        "\n",
        "#### Passo 3 - Instalando o Apache Spark \n",
        "\n",
        "Selecione a versão mais estável clicando [aqui](http://spark.apache.org/downloads.html). Na criação deste projeto utilizamos a versão do Spark **3.1.2** e como tipo de pacote selecionamos **Pre-built for Apache Hadoop 2.7**.\n",
        "\n",
        "Para instalar o Apache Spark não é necessário executar um instalador, basta descomprimir os arquivos em uma pasta de sua escolha.\n",
        "\n",
        "<font color=red>Obs.: certifique-se de que o caminho onde os arquivos do Spark foram armazenados não contenham espaços (ex.: **\"C:/spark/spark-3.1.2-bin-hadoop2.7\"**).</font>\n",
        "\n",
        "Para testar o funcionamento do Spark execute os comandos abaixo em seu *prompt* de comando. Esses comandos assumem que você extraiu os arquivos do Spark na pasta **\"C:/spark/\"**.\n",
        "\n",
        "```\n",
        "cd C:/spark/spark-3.1.2-bin-hadoop2.7\n",
        "```\n",
        "\n",
        "```\n",
        "bin/pyspark\n",
        "```\n",
        "\n",
        "O comando acima inicia o *shell* do PySpark que permite trabalhar interativamente com o Spark.\n",
        "\n",
        "Para sair basta digitar `exit()` e logo depois presionar *Enter*. Para voltar ao *prompt* pressione *Enter* novamente.\n",
        "\n",
        "#### Passo 4 - Instalando o findspark\n",
        "\n",
        "```\n",
        "pip install findspark\n",
        "```\n",
        "\n",
        "#### Passo 5 - Instalando o winutils\n",
        "\n",
        "Os arquivos do Spark não incluem o utilitário **winutils.exe** que é utilizado pelo Spark no Windows. Se não informar onde o Spark deve procurar este utilitário, veremos alguns erros no console e também não conseguiremos executar *scripts* Python utilizando o utilitário `spark-submit`.\n",
        "\n",
        "Faça o [download](https://github.com/steveloughran/winutils) para a versão do Hadoop para a qual sua instalação do Spark foi construída. Em nosso exemplo foi utilizada a [versão 2.7](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin). Faça o *download* apenas do arquivo **winutils.exe**.\n",
        "\n",
        "Crie a pasta **\"hadoop/bin\"** dentro da pasta que contém os arquivos do Spark (em nosso exemplo **\"C:/spark/spark-3.1.2-bin-hadoop2.7\"**) e copie o arquivo **winutils.exe** para dentro desta pasta.\n",
        "\n",
        "Crie duas variáveis de ambiente no seu Windows. A primeira chamada **SPARK_HOME** que aponta para a pasta onde os arquivos Spark foram armazenados (em nosso exemplo **\"C:/spark/spark-3.1.2-bin-hadoop2.7\"**). A segunda chamada **HADOOP_HOME** que aponta para **%SPARK_HOME%/hadoop** (assim podemos modificar **SPARK_HOME** sem precisar alterar **HADOOP_HOME**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_pds0odi1Rj"
      },
      "source": [
        "## Utilizando o Spark no Google Colab\n",
        "\n",
        "Para facilitar o desenvolvimento de nosso projeto neste curso vamos utilizar o Google Colab como ferramenta e para configurar o PySpark basta executar os comandos abaixo na própria célula do seu *notebook*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JmcO4uWZ3OLO"
      },
      "outputs": [],
      "source": [
        "# instalar as dependências\n",
        "# !apt-get update -qq\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "# !tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Eclipse Adoptium/jdk-17.0.6.10-hotspot/bin\"\n",
        "os.environ[\"SPARK_HOME\"] = \"C:/Users/tarsi/Spark/spark-3.3.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kqdft9fpGlH0"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZNca7Pjgqf"
      },
      "source": [
        "# Carregamento de Dados\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YGNS075GGlH0"
      },
      "source": [
        "## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
        "\n",
        "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
        "\n",
        "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrão de construtor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yayRGdn_GlH1"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder \\\n\u001b[0;32m      4\u001b[0m     \u001b[39m.\u001b[39;49mmaster(\u001b[39m'\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m'\u001b[39;49m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mIniciando com Spark\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m----> 6\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[1;32mC:\\Users/tarsi/Spark/spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    270\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
            "File \u001b[1;32mC:\\Users/tarsi/Spark/spark-3.3.1-bin-hadoop3\\python\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    484\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[1;32mC:\\Users/tarsi/Spark/spark-3.3.1-bin-hadoop3\\python\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",
            "File \u001b[1;32mC:\\Users/tarsi/Spark/spark-3.3.1-bin-hadoop3\\python\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
            "File \u001b[1;32mC:\\Users/tarsi/Spark/spark-3.3.1-bin-hadoop3\\python\\pyspark\\java_gateway.py:103\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 103\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"Iniciando com Spark\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PizPNqbmCuSM"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n2mFfLx470tO"
      },
      "source": [
        "## Acessando o [Spark UI](https://spark.apache.org/docs/3.1.2/web-ui.html) (Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpSjVUhR8AYq"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"Iniciando com Spark\") \\\n",
        "    .config('spark.ui.port', '4050')\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configurar para instalaçao no windows e pegar token no site"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Otwtgug3T4tW"
      },
      "source": [
        "[Site ngrok](https://ngrok.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy6EXcMn8bg5"
      },
      "outputs": [],
      "source": [
        "# !ngrok config add-authtoken 2EUMvorPW5JMdOagu5qLsij1v5H_2P6xC8oWQubhrbkjHomfi\n",
        "# !wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL5yysRL8i_I"
      },
      "outputs": [],
      "source": [
        "# get_ipython().system_raw('./ngrok http 4050 &')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOFKWePj8no0"
      },
      "outputs": [],
      "source": [
        "# !curl -s http://localhost:4040/api/tunnels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oonn_IJ08rfq"
      },
      "outputs": [],
      "source": [
        "# spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbOcx1oXGlH4"
      },
      "source": [
        "## DataFrames com Spark\n",
        "\n",
        "\n",
        "### Interfaces Spark\n",
        "\n",
        "Existem três interfaces principais do Apache Spark que você deve conhecer: Resilient Distributed Dataset, DataFrame e Dataset.\n",
        "\n",
        "- **Resilient Distributed Dataset**: A primeira abstração do Apache Spark foi o Resilient Distributed Dataset (RDD). É uma interface para uma sequência de objetos de dados que consiste em um ou mais tipos localizados em uma coleção de máquinas (um cluster). Os RDDs podem ser criados de várias maneiras e são a API de “nível mais baixo” disponível. Embora esta seja a estrutura de dados original do Apache Spark, você deve se concentrar na API DataFrame, que é um superconjunto da funcionalidade RDD. A API RDD está disponível nas linguagens Java, Python e Scala.\n",
        "\n",
        "- **DataFrame**: Trata-se de um conceito similar ao DataFrame que você pode estar familiarizado como o pacote pandas do Python e a linguagem R . A API DataFrame está disponível nas linguagens Java, Python, R e Scala.\n",
        "\n",
        "- **Dataset**: uma combinação de DataFrame e RDD. Ele fornece a interface digitada que está disponível em RDDs enquanto fornece a conveniência do DataFrame. A API Dataset está disponível nas linguagens Java e Scala.\n",
        "\n",
        "Em muitos cenários, especialmente com as otimizações de desempenho incorporadas em DataFrames e Datasets, não será necessário trabalhar com RDDs. Mas é importante entender a abstração RDD porque:\n",
        "\n",
        "- O RDD é a infraestrutura subjacente que permite que o Spark seja executado com tanta rapidez e forneça a linhagem de dados.\n",
        "\n",
        "- Se você estiver mergulhando em componentes mais avançados do Spark, pode ser necessário usar RDDs.\n",
        "\n",
        "- As visualizações na Spark UI fazem referência a RDDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpFfzZN0GlH5"
      },
      "outputs": [],
      "source": [
        "data = [('Zeca','35'), ('Eva', '29')]\n",
        "colNames = ['Nome', 'Idade']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.createDataFrame(data, colNames)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXVfnTIHGlH3"
      },
      "source": [
        "## Projeto\n",
        "\n",
        "Nosso projeto consiste em ler, manipular, tratar e salvar um conjunto de dados volumosos utilizando como ferramenta o Spark."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KCYYVAtjhMa7"
      },
      "source": [
        "## Carregamento de dados\n",
        "\n",
        "### Dados Públicos CNPJ\n",
        "#### Receita Federal\n",
        "\n",
        "> [Empresas](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)\n",
        "> \n",
        "> [Estabelecimentos](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)\n",
        "> \n",
        "> [Sócios](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)\n",
        "\n",
        "[Fonte original dos dados](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj)\n",
        "\n",
        "---\n",
        "[property SparkSession.read](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html)\n",
        "\n",
        "[DataFrameReader.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6znrrAdApQmE"
      },
      "source": [
        "### Montando nosso drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g2I8wYShfYJ"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f6bwFADGlH6"
      },
      "source": [
        "### Carregando os dados das empresas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"C:/Users/tasio.guimaraes/Documents/Spark/Batch/Data/empresas/part-00000-58983ad4-8444-4405-aec6-9cd3e5413d1b-c000.csv\"\n",
        "empresas = spark.read.csv(path, sep=';', inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "empresas.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0QOCzKVGlH7"
      },
      "source": [
        "## Faça como eu fiz: Estabelecimentos e Sócios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DXanOaDk1Hc"
      },
      "source": [
        "### Carregando os dados dos estabelecimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIwhrJTIGlH7"
      },
      "outputs": [],
      "source": [
        "path = \"C:/Users/tasio.guimaraes/Documents/Spark/Batch/Data/estabelecimentos/part-00000-701c2cc9-d9db-469f-be12-341c24a77308-c000.csv\"\n",
        "estabelecimentos = spark.read.csv(path, sep=';', inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpfe_ApWGlH7"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOylSh6PGlH8"
      },
      "source": [
        "### Carregando os dados dos sócios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97PLgACTGlH8"
      },
      "outputs": [],
      "source": [
        "path = \"C:/Users/tasio.guimaraes/Documents/Spark/Batch/Data/socios/part-00000-1ab6459b-5a3c-4294-83b8-316c171f0202-c000.csv\"\n",
        "socios = spark.read.csv(path, sep=';', inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94dmXGICGlH8"
      },
      "outputs": [],
      "source": [
        "socios.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyeUU1CsGlH9"
      },
      "source": [
        "# Manipulando os Dados\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvN_LyAGlH9"
      },
      "source": [
        "## Operações básicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc8_B1H8iNkX"
      },
      "outputs": [],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f7djxh5GlH9"
      },
      "source": [
        "### Renomeando as colunas do DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg0w9tDlGlH-"
      },
      "outputs": [],
      "source": [
        "empresasColNames = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gYSVAgPk2h8"
      },
      "outputs": [],
      "source": [
        "for index,colName in enumerate(empresasColNames):\n",
        "        empresas = empresas.withColumnRenamed(f\"_c{index}\", colName)\n",
        "\n",
        "empresas.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMfPDezDGlH-"
      },
      "outputs": [],
      "source": [
        "estabsColNames = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a04WKP_dGlH_"
      },
      "outputs": [],
      "source": [
        "for index,colName in enumerate(estabsColNames):\n",
        "        estabelecimentos = estabelecimentos.withColumnRenamed(f\"_c{index}\", colName)\n",
        "\n",
        "estabelecimentos.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hR0XZghGlH_"
      },
      "outputs": [],
      "source": [
        "sociosColNames = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjBME0tPGlH_"
      },
      "outputs": [],
      "source": [
        "for index,colName in enumerate(sociosColNames):\n",
        "        socios = socios.withColumnRenamed(f\"_c{index}\", colName)\n",
        "\n",
        "socios.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EnibIqSUGlH_"
      },
      "source": [
        "## Analisando os dados\n",
        "\n",
        "[Data Types](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#data-types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BCEdVduXqLP"
      },
      "outputs": [],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aocivOoxGlIA"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNQJP7E6UsGC"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUXixnpzZPQe"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43jdqE5gGlIA"
      },
      "outputs": [],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVXz5ZFqY1sW"
      },
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A75QicR-WYxn"
      },
      "source": [
        "## Modificando os tipos de dados\n",
        "\n",
        "[Functions](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#functions)\n",
        "\n",
        "[withColumn](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVElbxNlfFjk"
      },
      "source": [
        "### Convertendo String ➔ Double\n",
        "\n",
        "#### `StringType ➔ DoubleType`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSDekv4rbodk"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType, StringType\n",
        "from pyspark.sql import functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjbORHHw4M5j"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyIP_6ge4ZF4"
      },
      "outputs": [],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUdfkV6EeCJt"
      },
      "outputs": [],
      "source": [
        "empresas = empresas.withColumn('capital_social_da_empresa', empresas['capital_social_da_empresa'].cast(DoubleType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsNOpcZoe20V"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jp_Zv8tAgcbN"
      },
      "source": [
        "### Convertendo String ➔ Date\n",
        "\n",
        "#### `StringType ➔ DateType`\n",
        "\n",
        "[Datetime Patterns](https://spark.apache.org/docs/3.1.2/sql-ref-datetime-pattern.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRdCvl26o1eC"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([(20200924,), (20201022,), (20210215,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OMaiBT16YAX"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4alYEILpRZe"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"data\", f.to_date(df.data.cast(StringType()), 'yyyyMMdd'))\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxB1k7IGp2vo"
      },
      "outputs": [],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yBmkN9H2-QP"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgO66CVEUGns"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L2904mRn5dy"
      },
      "outputs": [],
      "source": [
        "estabelecimentos = estabelecimentos\\\n",
        "    .withColumn(\n",
        "    \"data_situacao_cadastral\",\n",
        "    f.to_date(estabelecimentos.data_situacao_cadastral.cast(StringType()), 'yyyyMMdd')\n",
        "    )\\\n",
        "    .withColumn(\n",
        "    \"data_de_inicio_atividade\",\n",
        "    f.to_date(estabelecimentos.data_de_inicio_atividade.cast(StringType()), 'yyyyMMdd')\n",
        "    )\\\n",
        "    .withColumn(\n",
        "    \"data_da_situacao_especial\",\n",
        "    f.to_date(estabelecimentos.data_da_situacao_especial.cast(StringType()), 'yyyyMMdd')\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39H5_TRgV0j5"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "socios = socios\\\n",
        "    .withColumn(\n",
        "    \"data_de_entrada_sociedade\",\n",
        "    f.to_date(socios.data_de_entrada_sociedade.cast(StringType()), 'yyyyMMdd')\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg0lldBjGlIB"
      },
      "source": [
        "# Seleções e consultas\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H3q8yA8kgH9G"
      },
      "source": [
        "## Selecionando informações\n",
        " \n",
        "[DataFrame.select(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.select.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdMi5XrjbNxA"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select('*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9cgyry3GlIB"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "        .select('*')\\\n",
        "        .show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ilkFXsIbw9j"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "        .select('natureza_juridica', 'porte_da_empresa', 'capital_social_da_empresa')\\\n",
        "        .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estabelecimentos\\\n",
        "    .select('nome_fantasia', 'situacao_cadastral',  f.year('data_situacao_cadastral').alias('ano_de_entrada'))\\\n",
        "    .show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j94bdu9r0ykx"
      },
      "source": [
        "## Faça como eu fiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smUC0SgN0sGc"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    ('GISELLE PAULA GUIMARAES CASTRO', 15),\n",
        "    ('ELAINE GARCIA DE OLIVEIRA', 22),\n",
        "    ('JOAO CARLOS ABNER DE LOURDES', 43),\n",
        "    ('MARTA ZELI FERREIRA', 24),\n",
        "    ('LAUDENETE WIGGERS ROEDER', 51)\n",
        "]\n",
        "colNames = ['nome', 'idade']\n",
        "df = spark.createDataFrame(data, colNames)\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df \\\n",
        "    .select(\n",
        "        f.concat_ws(\n",
        "            ', ', \n",
        "            f.substring_index('nome', ' ', -1), \n",
        "            f.substring_index('nome', ' ', 1)\n",
        "        ).alias('ident'), \n",
        "        'idade') \\\n",
        "    .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS3mrMkHZjqX"
      },
      "source": [
        "## Identificando valores nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8orupk7E9sfp"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([(1,), (2,), (3,), (None,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l4asr_S95MN"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYjUKTUa_KzT"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([(1.,), (2.,), (3.,), (float('nan'),)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqzQebm7_QAO"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8v6WOpb96qU"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([('1',), ('2',), ('3',), (None,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nokz_PPC994j"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1wvIzb8-Rpk"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.select()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i95DWXnV-Usy"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.select([f.count(f.when(f.isnull(c),1)).alias(c) for c in estabelecimentos.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DrHfT6vbSeU"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estabelecimentos.na.fill(0).limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estabelecimentos.na.fill('-').limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estabelecimentos.na.fill('-').limit(5).toPandas()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m_NMPijDgp97"
      },
      "source": [
        "## Ordenando os dados\n",
        "\n",
        "[DataFrame.orderBy(*cols, **kwargs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiEEP4eTd9K_"
      },
      "outputs": [],
      "source": [
        "estabelecimentos\\\n",
        "    .select('nome_fantasia', 'situacao_cadastral',  f.year('data_situacao_cadastral').alias('ano_de_entrada')) \\\n",
        "    .orderBy('ano_de_entrada',ascending=False) \\\n",
        "    .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZrd_p7mfFDR"
      },
      "outputs": [],
      "source": [
        "estabelecimentos\\\n",
        "    .select('nome_fantasia', 'situacao_cadastral',  f.year('data_situacao_cadastral').alias('ano_de_entrada')) \\\n",
        "    .orderBy(['ano_de_entrada','situacao_cadastral'],ascending=[False,False]) \\\n",
        "    .show(5, False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DeMk86UGhZvc"
      },
      "source": [
        "## Filtrando os dados\n",
        "\n",
        "[DataFrame.where(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.where.html) ou [DataFrame.filter(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.filter.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6ZHVYBHjPa5"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .where(\"capital_social_da_empresa==50\") \\\n",
        "    .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6tE7LiKXT1I"
      },
      "outputs": [],
      "source": [
        "socios\\\n",
        "    .select('nome_do_socio_ou_razao_social')\\\n",
        "    .filter(socios.nome_do_socio_ou_razao_social.startswith(\"RODRIGO\"))\\\n",
        "    .filter(socios.nome_do_socio_ou_razao_social.endswith(\"DIAS\"))\\\n",
        "    .limit(10)\\\n",
        "    .toPandas()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ_FrfbOK-1a"
      },
      "source": [
        "## O comando LIKE\n",
        "\n",
        "[Column.like(other)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.like.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3sMWIEIQLY7"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([('RESTAURANTE DO RUI',), ('Juca restaurantes ltda',), ('Joca Restaurante',)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfc1p_pEQ1UJ"
      },
      "outputs": [],
      "source": [
        "df\\\n",
        "    .where(f.upper(df.data).like('%RESTAURANTE%'))\\\n",
        "    .show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df\\\n",
        "#     .filter(df.nome.like('C%'))\\\n",
        "#     .show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHqcmjIxGlIF"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select('razao_social_nome_empresarial', 'natureza_juridica', 'porte_da_empresa', 'capital_social_da_empresa')\\\n",
        "    .filter(f.upper(empresas['razao_social_nome_empresarial']).like('%RESTAURANTE%'))\\\n",
        "    .show(15, False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uiyZCFaEQPtQ"
      },
      "source": [
        "# Agregações e Junções\n",
        "---\n",
        "\n",
        "[DataFrame.groupBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)\n",
        "\n",
        "[DataFrame.agg(*exprs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.agg.html)\n",
        "\n",
        "[DataFrame.summary(*statistics)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html)\n",
        "\n",
        "> Funções:\n",
        "[approx_count_distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html) | \n",
        "[avg](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.avg.html) | \n",
        "[collect_list](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_list.html) | \n",
        "[collect_set](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_set.html) | \n",
        "[countDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.countDistinct.html) | \n",
        "[count](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.count.html) | \n",
        "[grouping](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.grouping.html) | \n",
        "[first](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.first.html) | \n",
        "[last](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.last.html) | \n",
        "[kurtosis](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.kurtosis.html) | \n",
        "[max](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.max.html) | \n",
        "[min](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.min.html) | \n",
        "[mean](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.mean.html) | \n",
        "[skewness](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.skewness.html) | \n",
        "[stddev ou stddev_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev.html) | \n",
        "[stddev_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev_pop.html) | \n",
        "[sum](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sum.html) | \n",
        "[sumDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sumDistinct.html) | \n",
        "[variance ou var_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.variance.html) | \n",
        "[var_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.var_pop.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc6YDFOkywQc"
      },
      "source": [
        "## Sumarizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8p8BS7QQKfS"
      },
      "outputs": [],
      "source": [
        "socios\\\n",
        "    .select(f.year('data_de_entrada_sociedade').alias('ano_de_entrada'))\\\n",
        "    .where('ano_de_entrada >= 2010')\\\n",
        "    .groupBy('ano_de_entrada')\\\n",
        "    .count()\\\n",
        "    .orderBy('ano_de_entrada', ascending=True)\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlAbbR4SdZHw"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select('cnpj_basico', 'porte_da_empresa', 'capital_social_da_empresa')\\\n",
        "    .groupBy('porte_da_empresa')\\\n",
        "    .agg(\n",
        "        f.avg(\"capital_social_da_empresa\").alias(\"capital_social_medio\"),\n",
        "        f.count(\"cnpj_basico\").alias(\"frequencia\")\n",
        "    )\\\n",
        "    .orderBy('porte_da_empresa', ascending=True)\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oOOQPUqNiRZ"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select(\"capital_social_da_empresa\")\\\n",
        "    .summary()\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [\n",
        "    ('CARLOS', 'MATEMÁTICA', 7), \n",
        "    ('IVO', 'MATEMÁTICA', 9), \n",
        "    ('MÁRCIA', 'MATEMÁTICA', 8), \n",
        "    ('LEILA', 'MATEMÁTICA', 9), \n",
        "    ('BRENO', 'MATEMÁTICA', 7), \n",
        "    ('LETÍCIA', 'MATEMÁTICA', 8), \n",
        "    ('CARLOS', 'FÍSICA', 2), \n",
        "    ('IVO', 'FÍSICA', 8), \n",
        "    ('MÁRCIA', 'FÍSICA', 10), \n",
        "    ('LEILA', 'FÍSICA', 9), \n",
        "    ('BRENO', 'FÍSICA', 1), \n",
        "    ('LETÍCIA', 'FÍSICA', 6), \n",
        "    ('CARLOS', 'QUÍMICA', 10), \n",
        "    ('IVO', 'QUÍMICA', 8), \n",
        "    ('MÁRCIA', 'QUÍMICA', 1), \n",
        "    ('LEILA', 'QUÍMICA', 10), \n",
        "    ('BRENO', 'QUÍMICA', 7), \n",
        "    ('LETÍCIA', 'QUÍMICA', 9)\n",
        "]\n",
        "colNames = ['nome', 'materia', 'nota']\n",
        "df = spark.createDataFrame(data, colNames)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.withColumn('status', f.when(df.nota >= 7, \"APROVADO\").otherwise(\"REPROVADO\"))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df\\\n",
        "    .select('nota')\\\n",
        "    .summary('min', '25%', '50%', '75%', 'max')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df\\\n",
        "    .groupBy('status')\\\n",
        "    .count()\\\n",
        "    .orderBy('status', ascending=True)\\\n",
        "    .show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rOCWSMepX5jN"
      },
      "source": [
        "## Juntando DataFrames - Joins\n",
        "\n",
        "[DataFrame.join(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uklRCkDX4Tf"
      },
      "outputs": [],
      "source": [
        "produtos = spark.createDataFrame(\n",
        "    [\n",
        "        ('1', 'Bebidas', 'Água mineral'), \n",
        "        ('2', 'Limpeza', 'Sabão em pó'), \n",
        "        ('3', 'Frios', 'Queijo'), \n",
        "        ('4', 'Bebidas', 'Refrigerante'),\n",
        "        ('5', 'Pet', 'Ração para cães')\n",
        "    ],\n",
        "    ['id', 'cat', 'prod']\n",
        ")\n",
        "\n",
        "impostos = spark.createDataFrame(\n",
        "    [\n",
        "        ('Bebidas', 0.15), \n",
        "        ('Limpeza', 0.05),\n",
        "        ('Frios', 0.065),\n",
        "        ('Carnes', 0.08)\n",
        "    ],\n",
        "    ['cat', 'tax']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCMHBCrokoc3"
      },
      "outputs": [],
      "source": [
        "produtos.join(impostos, 'cat', how='inner')\\\n",
        "    .sort('id')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "produtos.join(impostos, 'cat', how='left')\\\n",
        "    .sort('id')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "produtos.join(impostos, 'cat', how='right')\\\n",
        "    .sort('id')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "produtos.join(impostos, 'cat', how='outer')\\\n",
        "    .sort('id')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDaN4XFQkdsE"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyvClbqRHviC"
      },
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq_pGKucIjri"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWoVxvYKJkC-"
      },
      "outputs": [],
      "source": [
        "empresas_join = estabelecimentos.join(empresas, 'cnpj_basico', how='inner')\n",
        "empresas_join.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta9UsinQJn1w"
      },
      "outputs": [],
      "source": [
        "freq = empresas_join\\\n",
        "    .select(\n",
        "        'cnpj_basico', \n",
        "        f.year('data_de_inicio_atividade').alias('data_de_inicio')\n",
        "    )\\\n",
        "    .where('data_de_inicio >= 2010')\\\n",
        "    .groupBy('data_de_inicio')\\\n",
        "    .agg(f.count(\"cnpj_basico\").alias(\"frequencia\"))\\\n",
        "    .orderBy('data_de_inicio', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kki6ZPfNJ_sI"
      },
      "outputs": [],
      "source": [
        "freq.union(\n",
        "    freq.select(\n",
        "        f.lit('Total').alias('data_de_inicio'),\n",
        "        f.sum(freq.frequencia).alias('frequencia')   \n",
        "    )\n",
        ").show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq-9n_n2GlIF"
      },
      "source": [
        "## SparkSQL\n",
        "\n",
        "[SparkSession.sql(sqlQuery)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.sql.html)\n",
        "\n",
        "Para saber mais sobre performance: [Artigo - Spark RDDs vs DataFrames vs SparkSQL](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGCFVpPnGlIF"
      },
      "outputs": [],
      "source": [
        "empresas.createOrReplaceTempView(\"empresasView\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c_OrvMLGlIF"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM empresasView\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cff25Y3GlIG"
      },
      "outputs": [],
      "source": [
        "spark\\\n",
        "    .sql(\"\"\"\n",
        "        SELECT * \n",
        "            FROM empresasView \n",
        "            WHERE qualificacao_do_responsavel = 50\n",
        "    \"\"\")\\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj0ADzBfGlIG"
      },
      "outputs": [],
      "source": [
        "spark\\\n",
        "    .sql(\"\"\"\n",
        "        SELECT porte_da_empresa, MEAN(capital_social_da_empresa) AS Media \n",
        "            FROM empresasView \n",
        "            GROUP BY porte_da_empresa\n",
        "    \"\"\")\\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRxV3Crg1ZFo"
      },
      "outputs": [],
      "source": [
        "empresas_join.createOrReplaceTempView(\"empresasJoinView\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0zjZOxM0jzy"
      },
      "outputs": [],
      "source": [
        "freq = spark\\\n",
        "    .sql(\"\"\"\n",
        "        SELECT YEAR(data_de_inicio_atividade) AS data_de_inicio, COUNT(cnpj_basico) AS count\n",
        "            FROM empresasJoinView \n",
        "            WHERE YEAR(data_de_inicio_atividade) >= 2010\n",
        "            GROUP BY data_de_inicio\n",
        "            ORDER BY data_de_inicio\n",
        "    \"\"\")\n",
        "\n",
        "freq\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0oUrvo4E76z"
      },
      "outputs": [],
      "source": [
        "freq.createOrReplaceTempView(\"freqView\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghAEAvfj0eS3"
      },
      "outputs": [],
      "source": [
        "spark\\\n",
        "    .sql(\"\"\"\n",
        "        SELECT *\n",
        "            FROM freqView\n",
        "        UNION ALL\n",
        "        SELECT 'Total' AS data_de_inicio, SUM(count) AS count\n",
        "            FROM freqView\n",
        "    \"\"\")\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "empresas_join\\\n",
        "    .select(f.year(empresas_join.data_de_inicio_atividade).alias('data_de_inicio'))\\\n",
        "    .where(\"data_de_inicio >= 2010\")\\\n",
        "    .groupBy('data_de_inicio')\\\n",
        "    .count()\\\n",
        "    .orderBy('data_de_inicio')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDM3l-CUGlIJ"
      },
      "source": [
        "# Formas de Armazenamento\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ySrkXajNGlIK"
      },
      "source": [
        "## Arquivos CSV\n",
        "\n",
        "[property DataFrame.write](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.write.html)\n",
        "\n",
        "[DataFrameWriter.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1PBgc6kGlIK"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .write\\\n",
        "    .csv(\n",
        "        path='C:/Users/tasio.guimaraes/Documents/Spark/Batch/Data/csv',\n",
        "        mode='overwrite'\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ge7vmWujGlIK"
      },
      "source": [
        "## Arquivos PARQUET\n",
        "\n",
        "[Apache Parquet](https://parquet.apache.org/)\n",
        "\n",
        "[DataFrameWriter.parquet(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42_1Y8eW6e2r"
      },
      "outputs": [],
      "source": [
        "empresas.write.parquet(\n",
        "    path='C:/Users/tasio.guimaraes/Documents/Spark/Batch/Data/parquet',\n",
        "    mode='overwrite',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD01X0MN6pgp"
      },
      "outputs": [],
      "source": [
        "empresas_parquet = spark.read.parquet(\n",
        "    'C:/Users/tasio.guimaraes/Documents/Spark/Batch/Data/parquet'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "empresas_parquet.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "empresas.write.parquet('/content/drive/MyDrive/data/empresas/parquet/')\n",
        "empresas.write.orc('/content/drive/MyDrive/data/empresas/orc/')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UYsfCfudGlIK"
      },
      "source": [
        "## Particionamento dos dados\n",
        "\n",
        "[DataFrameWriter.partitionBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3lGYb4qRVz3"
      },
      "outputs": [],
      "source": [
        "empresas.write.parquet(\n",
        "    path='/content/drive/MyDrive/curso-spark/empresas/parquet-partitionBy',\n",
        "    mode='overwrite',\n",
        "        partitionBy='porte_da_empresa'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Tv9DelGlIL"
      },
      "outputs": [],
      "source": [
        "empresas.coalesce(1).write.csv(\n",
        "    path='/content/drive/MyDrive/curso-spark/empresas/csv-unico',\n",
        "    mode='overwrite',\n",
        "    sep=';',\n",
        "    header=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "3198c22acf028788dcd3aa44a9905fe12d682b6b89cbef930b6baebab607d818"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
